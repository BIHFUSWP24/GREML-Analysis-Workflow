Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Job stats:
job           count
----------  -------
all               1
grm_to_csv        1
total             2

Select jobs to execute...

[Wed Mar 20 18:24:09 2024]
rule grm_to_csv:
    input: /home/lesi11/testData/test.grm.bin
    output: /home/lesi11/testData/test_size=100.grm.csv
    jobid: 3
    reason: Missing output files: /home/lesi11/testData/test_size=100.grm.csv
    wildcards: name=test, size=100
    resources: tmpdir=/tmp

[Wed Mar 20 18:24:10 2024]
Error in rule grm_to_csv:
    jobid: 3
    input: /home/lesi11/testData/test.grm.bin
    output: /home/lesi11/testData/test_size=100.grm.csv

RuleException:
CalledProcessError in file /sc-projects/sc-proj-dh-ag-eils-ml/workflow/rules/grm.smk, line 24:
Command 'set -euo pipefail;  Rscript --vanilla /sc-projects/sc-proj-dh-ag-eils-ml/workflow/.snakemake/scripts/tmpnn794z2g.grm_io.R' returned non-zero exit status 1.
  File "/sc-projects/sc-proj-dh-ag-eils-ml/workflow/rules/grm.smk", line 24, in __rule_grm_to_csv
  File "/home/lesi11/.conda/envs/greml/lib/python3.9/concurrent/futures/thread.py", line 58, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-03-20T182406.962436.snakemake.log
